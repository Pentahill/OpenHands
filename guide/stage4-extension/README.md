# é˜¶æ®µå››ï¼šæ‰©å±•å¼€å‘ï¼ˆ4-6å‘¨ï¼‰

## ğŸ¯ å­¦ä¹ ç›®æ ‡

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ å°†ï¼š
- å¼€å‘è‡ªå®šä¹‰å·¥å…·å’ŒAgentæ‰©å±•
- åˆ›å»ºä¸“ä¸šåŒ–çš„å¾®ä»£ç†ç³»ç»Ÿ
- æ‰©å±•è¿è¡Œæ—¶ç¯å¢ƒæ”¯æŒ
- å¼€å‘å®Œæ•´çš„æ’ä»¶ç³»ç»Ÿ
- æŒæ¡ç³»ç»Ÿé›†æˆå’Œæ‰©å±•æŠ€æœ¯

## ğŸ“‹ å­¦ä¹ æ¸…å•

### ç¬¬1-7å¤©ï¼šè‡ªå®šä¹‰å·¥å…·å¼€å‘
- [ ] ç†è§£å·¥å…·ç³»ç»Ÿçš„è®¾è®¡æ¨¡å¼
- [ ] å¼€å‘åŸºç¡€å·¥å…·ç±»
- [ ] å®ç°å¤æ‚å·¥å…·åŠŸèƒ½
- [ ] é›†æˆå¤–éƒ¨APIå’ŒæœåŠ¡
- [ ] ä¼˜åŒ–å·¥å…·æ€§èƒ½å’Œé”™è¯¯å¤„ç†

### ç¬¬8-14å¤©ï¼šå¾®ä»£ç†å¼€å‘
- [ ] æ·±å…¥ç†è§£å¾®ä»£ç†æ¶æ„
- [ ] åˆ›å»ºé¢†åŸŸç‰¹å®šå¾®ä»£ç†
- [ ] å®ç°è§¦å‘æœºåˆ¶å’Œä¸Šä¸‹æ–‡ç®¡ç†
- [ ] å¼€å‘å¾®ä»£ç†æ³¨å†Œå’Œå‘ç°ç³»ç»Ÿ
- [ ] ä¼˜åŒ–å¾®ä»£ç†æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§

### ç¬¬15-21å¤©ï¼šè¿è¡Œæ—¶æ‰©å±•
- [ ] åˆ†æç°æœ‰è¿è¡Œæ—¶æ¶æ„
- [ ] è®¾è®¡è‡ªå®šä¹‰è¿è¡Œæ—¶æ¥å£
- [ ] å®ç°ç‰¹å®šç¯å¢ƒæ”¯æŒ
- [ ] é›†æˆå®‰å…¨å’Œç›‘æ§æœºåˆ¶
- [ ] æµ‹è¯•å’Œä¼˜åŒ–è¿è¡Œæ—¶æ€§èƒ½

### ç¬¬22-28å¤©ï¼šæ’ä»¶ç³»ç»Ÿå¼€å‘
- [ ] è®¾è®¡æ’ä»¶æ¶æ„å’Œæ¥å£
- [ ] å®ç°æ’ä»¶åŠ è½½å’Œç®¡ç†
- [ ] å¼€å‘æ’ä»¶é€šä¿¡æœºåˆ¶
- [ ] åˆ›å»ºæ’ä»¶å¼€å‘å·¥å…·
- [ ] å»ºç«‹æ’ä»¶ç”Ÿæ€ç³»ç»Ÿ

### ç¬¬29-42å¤©ï¼šç³»ç»Ÿé›†æˆå’Œä¼˜åŒ–
- [ ] é›†æˆæ‰€æœ‰æ‰©å±•ç»„ä»¶
- [ ] ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½å’Œç¨³å®šæ€§
- [ ] å®ç°å®Œæ•´çš„æµ‹è¯•è¦†ç›–
- [ ] ç¼–å†™è¯¦ç»†çš„æ–‡æ¡£å’Œç¤ºä¾‹
- [ ] å‡†å¤‡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

## ğŸ“š æ‰©å±•å¼€å‘æ ¸å¿ƒæ¦‚å¿µ

### 1. å·¥å…·ç³»ç»Ÿæ¶æ„
```
å·¥å…·ç³»ç»Ÿ
â”œâ”€â”€ å·¥å…·åŸºç±» (BaseTool)
â”œâ”€â”€ å·¥å…·æ³¨å†Œå™¨ (ToolRegistry)
â”œâ”€â”€ å·¥å…·æ‰§è¡Œå™¨ (ToolExecutor)
â”œâ”€â”€ å·¥å…·ç®¡ç†å™¨ (ToolManager)
â””â”€â”€ å·¥å…·æ’ä»¶ (ToolPlugin)
```

### 2. å¾®ä»£ç†ç³»ç»Ÿæ¶æ„
```
å¾®ä»£ç†ç³»ç»Ÿ
â”œâ”€â”€ å¾®ä»£ç†åŸºç±» (BaseMicroagent)
â”œâ”€â”€ è§¦å‘å™¨ç³»ç»Ÿ (TriggerSystem)
â”œâ”€â”€ ä¸Šä¸‹æ–‡ç®¡ç† (ContextManager)
â”œâ”€â”€ çŸ¥è¯†åº“é›†æˆ (KnowledgeBase)
â””â”€â”€ åŠ¨æ€åŠ è½½å™¨ (DynamicLoader)
```

### 3. è¿è¡Œæ—¶æ‰©å±•æ¶æ„
```
è¿è¡Œæ—¶æ‰©å±•
â”œâ”€â”€ è¿è¡Œæ—¶æ¥å£ (RuntimeInterface)
â”œâ”€â”€ ç¯å¢ƒé€‚é…å™¨ (EnvironmentAdapter)
â”œâ”€â”€ èµ„æºç®¡ç†å™¨ (ResourceManager)
â”œâ”€â”€ å®‰å…¨æ§åˆ¶å™¨ (SecurityController)
â””â”€â”€ ç›‘æ§é›†æˆ (MonitoringIntegration)
```

## ğŸ› ï¸ å®è·µé¡¹ç›®

### é¡¹ç›®1ï¼šé«˜çº§æ•°æ®åˆ†æå·¥å…·
åˆ›å»ºä¸€ä¸ªé›†æˆå¤šç§æ•°æ®åˆ†æåŠŸèƒ½çš„å·¥å…·ï¼š

```python
# advanced_data_tool.py
from typing import Dict, List, Any, Optional, Union
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import plotly.express as px
import plotly.graph_objects as go
from openhands.events.action import Action
from openhands.events.observation import Observation

class AdvancedDataAnalysisTool:
    """é«˜çº§æ•°æ®åˆ†æå·¥å…·"""

    def __init__(self):
        self.name = "advanced_data_analysis"
        self.description = "Advanced data analysis and visualization tool"
        self.supported_formats = ['csv', 'json', 'excel', 'parquet']
        self.analysis_cache = {}

    async def execute(self, action: Action) -> Observation:
        """æ‰§è¡Œæ•°æ®åˆ†æä»»åŠ¡"""
        try:
            task_type = action.task_type
            data_source = action.data_source
            parameters = action.parameters or {}

            # åŠ è½½æ•°æ®
            data = await self._load_data(data_source)

            # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œåˆ†æ
            if task_type == "exploratory_analysis":
                result = await self._exploratory_analysis(data, parameters)
            elif task_type == "statistical_analysis":
                result = await self._statistical_analysis(data, parameters)
            elif task_type == "machine_learning":
                result = await self._machine_learning_analysis(data, parameters)
            elif task_type == "visualization":
                result = await self._create_visualizations(data, parameters)
            elif task_type == "data_cleaning":
                result = await self._clean_data(data, parameters)
            else:
                raise ValueError(f"Unsupported task type: {task_type}")

            return Observation(
                content=result,
                observation="DataAnalysisObservation",
                success=True
            )

        except Exception as e:
            return Observation(
                content=f"Data analysis failed: {str(e)}",
                observation="ErrorObservation",
                success=False
            )

    async def _load_data(self, data_source: Union[str, Dict]) -> pd.DataFrame:
        """åŠ è½½æ•°æ®"""
        if isinstance(data_source, str):
            # ä»æ–‡ä»¶è·¯å¾„åŠ è½½
            if data_source.endswith('.csv'):
                return pd.read_csv(data_source)
            elif data_source.endswith('.json'):
                return pd.read_json(data_source)
            elif data_source.endswith(('.xlsx', '.xls')):
                return pd.read_excel(data_source)
            elif data_source.endswith('.parquet'):
                return pd.read_parquet(data_source)
            else:
                raise ValueError(f"Unsupported file format: {data_source}")
        elif isinstance(data_source, dict):
            # ä»å­—å…¸åˆ›å»ºDataFrame
            return pd.DataFrame(data_source)
        else:
            raise ValueError("Invalid data source format")

    async def _exploratory_analysis(self, data: pd.DataFrame, params: Dict) -> Dict:
        """æ¢ç´¢æ€§æ•°æ®åˆ†æ"""
        analysis = {
            'basic_info': {
                'shape': data.shape,
                'columns': list(data.columns),
                'dtypes': data.dtypes.to_dict(),
                'memory_usage': data.memory_usage(deep=True).sum()
            },
            'missing_values': data.isnull().sum().to_dict(),
            'summary_statistics': data.describe().to_dict(),
            'unique_values': {col: data[col].nunique() for col in data.columns},
            'correlations': data.corr().to_dict() if data.select_dtypes(include=[np.number]).shape[1] > 1 else {}
        }

        # æ£€æµ‹å¼‚å¸¸å€¼
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        outliers = {}
        for col in numeric_columns:
            Q1 = data[col].quantile(0.25)
            Q3 = data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers[col] = {
                'count': ((data[col] < lower_bound) | (data[col] > upper_bound)).sum(),
                'percentage': ((data[col] < lower_bound) | (data[col] > upper_bound)).mean() * 100
            }

        analysis['outliers'] = outliers

        return analysis

    async def _statistical_analysis(self, data: pd.DataFrame, params: Dict) -> Dict:
        """ç»Ÿè®¡åˆ†æ"""
        from scipy import stats

        analysis = {}

        # å‡è®¾æ£€éªŒ
        if 'hypothesis_test' in params:
            test_type = params['hypothesis_test']['type']
            columns = params['hypothesis_test']['columns']

            if test_type == 't_test' and len(columns) == 2:
                # ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ
                group1 = data[columns[0]].dropna()
                group2 = data[columns[1]].dropna()
                t_stat, p_value = stats.ttest_ind(group1, group2)
                analysis['t_test'] = {
                    't_statistic': t_stat,
                    'p_value': p_value,
                    'significant': p_value < 0.05
                }

            elif test_type == 'chi_square' and len(columns) == 2:
                # å¡æ–¹æ£€éªŒ
                contingency_table = pd.crosstab(data[columns[0]], data[columns[1]])
                chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
                analysis['chi_square'] = {
                    'chi2_statistic': chi2,
                    'p_value': p_value,
                    'degrees_of_freedom': dof,
                    'significant': p_value < 0.05
                }

        # ç›¸å…³æ€§åˆ†æ
        if 'correlation_analysis' in params:
            method = params['correlation_analysis'].get('method', 'pearson')
            numeric_data = data.select_dtypes(include=[np.number])

            if method == 'pearson':
                corr_matrix = numeric_data.corr(method='pearson')
            elif method == 'spearman':
                corr_matrix = numeric_data.corr(method='spearman')
            else:
                corr_matrix = numeric_data.corr(method='kendall')

            analysis['correlation_matrix'] = corr_matrix.to_dict()

        return analysis

    async def _machine_learning_analysis(self, data: pd.DataFrame, params: Dict) -> Dict:
        """æœºå™¨å­¦ä¹ åˆ†æ"""
        ml_type = params.get('ml_type', 'classification')
        target_column = params.get('target_column')
        feature_columns = params.get('feature_columns', [])

        if not target_column or target_column not in data.columns:
            raise ValueError("Target column not specified or not found in data")

        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        if not feature_columns:
            feature_columns = [col for col in data.columns if col != target_column]

        X = data[feature_columns].select_dtypes(include=[np.number])
        y = data[target_column]

        # å¤„ç†ç¼ºå¤±å€¼
        X = X.fillna(X.mean())
        y = y.fillna(y.mode()[0] if not y.mode().empty else 0)

        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        analysis = {}

        if ml_type == 'classification':
            # åˆ†ç±»ä»»åŠ¡
            model = RandomForestClassifier(n_estimators=100, random_state=42)
            model.fit(X_train, y_train)

            # é¢„æµ‹å’Œè¯„ä¼°
            y_pred = model.predict(X_test)

            analysis['classification'] = {
                'accuracy': model.score(X_test, y_test),
                'feature_importance': dict(zip(X.columns, model.feature_importances_)),
                'classification_report': classification_report(y_test, y_pred, output_dict=True)
            }

        elif ml_type == 'regression':
            # å›å½’ä»»åŠ¡
            from sklearn.ensemble import RandomForestRegressor
            from sklearn.metrics import mean_squared_error, r2_score

            model = RandomForestRegressor(n_estimators=100, random_state=42)
            model.fit(X_train, y_train)

            y_pred = model.predict(X_test)

            analysis['regression'] = {
                'r2_score': r2_score(y_test, y_pred),
                'mse': mean_squared_error(y_test, y_pred),
                'feature_importance': dict(zip(X.columns, model.feature_importances_))
            }

        return analysis

    async def _create_visualizations(self, data: pd.DataFrame, params: Dict) -> Dict:
        """åˆ›å»ºå¯è§†åŒ–"""
        viz_types = params.get('visualization_types', ['histogram', 'scatter', 'correlation_heatmap'])
        output_format = params.get('output_format', 'html')

        visualizations = {}

        for viz_type in viz_types:
            if viz_type == 'histogram':
                # ç›´æ–¹å›¾
                numeric_columns = data.select_dtypes(include=[np.number]).columns[:5]  # é™åˆ¶æ•°é‡
                fig = px.histogram(data, x=numeric_columns[0] if len(numeric_columns) > 0 else data.columns[0])
                visualizations['histogram'] = fig.to_html() if output_format == 'html' else fig.to_json()

            elif viz_type == 'scatter':
                # æ•£ç‚¹å›¾
                numeric_columns = data.select_dtypes(include=[np.number]).columns
                if len(numeric_columns) >= 2:
                    fig = px.scatter(data, x=numeric_columns[0], y=numeric_columns[1])
                    visualizations['scatter'] = fig.to_html() if output_format == 'html' else fig.to_json()

            elif viz_type == 'correlation_heatmap':
                # ç›¸å…³æ€§çƒ­åŠ›å›¾
                numeric_data = data.select_dtypes(include=[np.number])
                if numeric_data.shape[1] > 1:
                    corr_matrix = numeric_data.corr()
                    fig = px.imshow(corr_matrix, text_auto=True, aspect="auto")
                    visualizations['correlation_heatmap'] = fig.to_html() if output_format == 'html' else fig.to_json()

            elif viz_type == 'box_plot':
                # ç®±çº¿å›¾
                numeric_columns = data.select_dtypes(include=[np.number]).columns[:3]
                for col in numeric_columns:
                    fig = px.box(data, y=col)
                    visualizations[f'box_plot_{col}'] = fig.to_html() if output_format == 'html' else fig.to_json()

        return visualizations

    async def _clean_data(self, data: pd.DataFrame, params: Dict) -> Dict:
        """æ•°æ®æ¸…æ´—"""
        cleaning_operations = params.get('operations', ['remove_duplicates', 'handle_missing', 'remove_outliers'])

        original_shape = data.shape
        cleaned_data = data.copy()
        operations_performed = []

        for operation in cleaning_operations:
            if operation == 'remove_duplicates':
                before_count = len(cleaned_data)
                cleaned_data = cleaned_data.drop_duplicates()
                after_count = len(cleaned_data)
                operations_performed.append({
                    'operation': 'remove_duplicates',
                    'removed_rows': before_count - after_count
                })

            elif operation == 'handle_missing':
                missing_strategy = params.get('missing_strategy', 'drop')
                missing_before = cleaned_data.isnull().sum().sum()

                if missing_strategy == 'drop':
                    cleaned_data = cleaned_data.dropna()
                elif missing_strategy == 'fill_mean':
                    numeric_columns = cleaned_data.select_dtypes(include=[np.number]).columns
                    cleaned_data[numeric_columns] = cleaned_data[numeric_columns].fillna(
                        cleaned_data[numeric_columns].mean()
                    )
                elif missing_strategy == 'fill_mode':
                    for col in cleaned_data.columns:
                        mode_value = cleaned_data[col].mode()
                        if not mode_value.empty:
                            cleaned_data[col] = cleaned_data[col].fillna(mode_value[0])

                missing_after = cleaned_data.isnull().sum().sum()
                operations_performed.append({
                    'operation': 'handle_missing',
                    'strategy': missing_strategy,
                    'missing_values_handled': missing_before - missing_after
                })

            elif operation == 'remove_outliers':
                numeric_columns = cleaned_data.select_dtypes(include=[np.number]).columns
                outliers_removed = 0

                for col in numeric_columns:
                    Q1 = cleaned_data[col].quantile(0.25)
                    Q3 = cleaned_data[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR

                    before_count = len(cleaned_data)
                    cleaned_data = cleaned_data[
                        (cleaned_data[col] >= lower_bound) &
                        (cleaned_data[col] <= upper_bound)
                    ]
                    outliers_removed += before_count - len(cleaned_data)

                operations_performed.append({
                    'operation': 'remove_outliers',
                    'outliers_removed': outliers_removed
                })

        # ä¿å­˜æ¸…æ´—åçš„æ•°æ®
        output_path = params.get('output_path', 'cleaned_data.csv')
        cleaned_data.to_csv(output_path, index=False)

        return {
            'original_shape': original_shape,
            'cleaned_shape': cleaned_data.shape,
            'operations_performed': operations_performed,
            'output_path': output_path,
            'data_quality_score': self._calculate_data_quality_score(cleaned_data)
        }

    def _calculate_data_quality_score(self, data: pd.DataFrame) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
        # ç®€å•çš„æ•°æ®è´¨é‡è¯„åˆ†ç®—æ³•
        completeness = 1 - (data.isnull().sum().sum() / (data.shape[0] * data.shape[1]))
        uniqueness = data.nunique().sum() / (data.shape[0] * data.shape[1])
        consistency = 1.0  # ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§

        quality_score = (completeness * 0.4 + uniqueness * 0.3 + consistency * 0.3) * 100
        return round(quality_score, 2)

# å·¥å…·æ³¨å†Œ
def register_advanced_data_tool():
    """æ³¨å†Œé«˜çº§æ•°æ®åˆ†æå·¥å…·"""
    tool = AdvancedDataAnalysisTool()
    return {
        'name': tool.name,
        'description': tool.description,
        'execute': tool.execute,
        'supported_formats': tool.supported_formats
    }
```

### é¡¹ç›®2ï¼šä¸“ä¸šåŒ–å¾®ä»£ç†å¼€å‘
åˆ›å»ºä¸€ä¸ªä¸“é—¨å¤„ç†ä»£ç å®¡æŸ¥çš„å¾®ä»£ç†ï¼š

```python
# code_review_microagent.py
from typing import Dict, List, Any, Optional
import ast
import re
from pathlib import Path
from openhands.microagent.microagent import Microagent
from openhands.core.message import Message

class CodeReviewMicroagent(Microagent):
    """ä»£ç å®¡æŸ¥å¾®ä»£ç†"""

    def __init__(self):
        super().__init__()
        self.name = "code_review_specialist"
        self.description = "Specialized microagent for code review and quality analysis"
        self.triggers = [
            "code review", "review code", "check code quality",
            "analyze code", "code analysis", "refactor suggestion"
        ]
        self.expertise_areas = [
            "code_quality", "security", "performance",
            "maintainability", "best_practices"
        ]
        self.supported_languages = [
            "python", "javascript", "typescript", "java",
            "go", "rust", "cpp", "csharp"
        ]

    async def process_request(self, message: Message, context: Dict) -> Dict:
        """å¤„ç†ä»£ç å®¡æŸ¥è¯·æ±‚"""
        try:
            # è§£æè¯·æ±‚
            request_info = await self._parse_request(message.content, context)

            # è·å–ä»£ç 
            code_content = await self._extract_code(request_info)

            # æ‰§è¡Œä»£ç å®¡æŸ¥
            review_results = await self._perform_code_review(
                code_content, request_info
            )

            # ç”Ÿæˆå®¡æŸ¥æŠ¥å‘Š
            report = await self._generate_review_report(review_results)

            return {
                'success': True,
                'report': report,
                'recommendations': review_results.get('recommendations', []),
                'metrics': review_results.get('metrics', {}),
                'priority_issues': review_results.get('priority_issues', [])
            }

        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'fallback_suggestions': await self._get_fallback_suggestions()
            }

    async def _parse_request(self, content: str, context: Dict) -> Dict:
        """è§£æå®¡æŸ¥è¯·æ±‚"""
        request_info = {
            'review_type': 'comprehensive',  # comprehensive, security, performance
            'language': 'python',  # é»˜è®¤è¯­è¨€
            'focus_areas': [],
            'code_source': None,
            'file_paths': []
        }

        # æ£€æµ‹ç¼–ç¨‹è¯­è¨€
        language_patterns = {
            'python': r'\.(py|pyw)$|python|def |class |import ',
            'javascript': r'\.(js|jsx)$|javascript|function |const |let ',
            'typescript': r'\.(ts|tsx)$|typescript|interface |type ',
            'java': r'\.(java)$|java|public class |private |protected ',
            'go': r'\.(go)$|golang|func |package |import ',
            'rust': r'\.(rs)$|rust|fn |struct |impl ',
            'cpp': r'\.(cpp|cc|cxx|h|hpp)$|c\+\+|#include|class |namespace ',
            'csharp': r'\.(cs)$|c#|csharp|public class |namespace |using '
        }

        for lang, pattern in language_patterns.items():
            if re.search(pattern, content, re.IGNORECASE):
                request_info['language'] = lang
                break

        # æ£€æµ‹å®¡æŸ¥ç±»å‹
        if any(keyword in content.lower() for keyword in ['security', 'vulnerability', 'exploit']):
            request_info['review_type'] = 'security'
            request_info['focus_areas'].append('security')

        if any(keyword in content.lower() for keyword in ['performance', 'optimization', 'speed']):
            request_info['review_type'] = 'performance'
            request_info['focus_areas'].append('performance')

        if any(keyword in content.lower() for keyword in ['maintainability', 'readability', 'clean']):
            request_info['focus_areas'].append('maintainability')

        # æå–æ–‡ä»¶è·¯å¾„
        file_path_pattern = r'([a-zA-Z0-9_/\\.-]+\.(py|js|ts|java|go|rs|cpp|cs|h|hpp))'
        file_paths = re.findall(file_path_pattern, content)
        request_info['file_paths'] = [path[0] for path in file_paths]

        return request_info

    async def _extract_code(self, request_info: Dict) -> Dict:
        """æå–ä»£ç å†…å®¹"""
        code_content = {}

        # ä»æ–‡ä»¶è·¯å¾„è¯»å–ä»£ç 
        for file_path in request_info['file_paths']:
            try:
                path = Path(file_path)
                if path.exists() and path.is_file():
                    with open(path, 'r', encoding='utf-8') as f:
                        code_content[str(path)] = {
                            'content': f.read(),
                            'language': self._detect_language_from_extension(path.suffix),
                            'size': path.stat().st_size
                        }
            except Exception as e:
                print(f"Error reading file {file_path}: {e}")

        return code_content

    async def _perform_code_review(self, code_content: Dict, request_info: Dict) -> Dict:
        """æ‰§è¡Œä»£ç å®¡æŸ¥"""
        review_results = {
            'issues': [],
            'recommendations': [],
            'metrics': {},
            'priority_issues': []
        }

        for file_path, file_info in code_content.items():
            content = file_info['content']
            language = file_info['language']

            # åŸºäºè¯­è¨€çš„ç‰¹å®šå®¡æŸ¥
            if language == 'python':
                file_results = await self._review_python_code(content, file_path)
            elif language in ['javascript', 'typescript']:
                file_results = await self._review_js_ts_code(content, file_path)
            else:
                file_results = await self._review_generic_code(content, file_path)

            # åˆå¹¶ç»“æœ
            review_results['issues'].extend(file_results.get('issues', []))
            review_results['recommendations'].extend(file_results.get('recommendations', []))
            review_results['metrics'][file_path] = file_results.get('metrics', {})

        # è¯†åˆ«ä¼˜å…ˆçº§é—®é¢˜
        review_results['priority_issues'] = await self._identify_priority_issues(
            review_results['issues']
        )

        return review_results

    async def _review_python_code(self, content: str, file_path: str) -> Dict:
        """å®¡æŸ¥Pythonä»£ç """
        issues = []
        recommendations = []
        metrics = {}

        try:
            # è§£æAST
            tree = ast.parse(content)

            # ä»£ç å¤æ‚åº¦åˆ†æ
            complexity = self._calculate_cyclomatic_complexity(tree)
            metrics['cyclomatic_complexity'] = complexity

            if complexity > 10:
                issues.append({
                    'type': 'complexity',
                    'severity': 'high',
                    'message': f'High cyclomatic complexity: {complexity}',
                    'file': file_path,
                    'suggestion': 'Consider breaking down complex functions into smaller ones'
                })

            # æ£€æŸ¥å‡½æ•°é•¿åº¦
            function_lengths = self._analyze_function_lengths(tree)
            for func_name, length in function_lengths.items():
                if length > 50:
                    issues.append({
                        'type': 'maintainability',
                        'severity': 'medium',
                        'message': f'Function {func_name} is too long ({length} lines)',
                        'file': file_path,
                        'suggestion': 'Consider breaking down the function into smaller functions'
                    })

            # æ£€æŸ¥å¯¼å…¥è¯­å¥
            import_issues = self._check_imports(tree)
            issues.extend(import_issues)

            # æ£€æŸ¥å‘½åè§„èŒƒ
            naming_issues = self._check_naming_conventions(tree)
            issues.extend(naming_issues)

            # å®‰å…¨æ£€æŸ¥
            security_issues = self._check_security_issues(content)
            issues.extend(security_issues)

            # æ€§èƒ½æ£€æŸ¥
            performance_issues = self._check_performance_issues(tree, content)
            issues.extend(performance_issues)

        except SyntaxError as e:
            issues.append({
                'type': 'syntax',
                'severity': 'critical',
                'message': f'Syntax error: {str(e)}',
                'file': file_path,
                'line': e.lineno
            })

        # ç”Ÿæˆå»ºè®®
        recommendations.extend(self._generate_python_recommendations(issues))

        return {
            'issues': issues,
            'recommendations': recommendations,
            'metrics': metrics
        }

    def _calculate_cyclomatic_complexity(self, tree: ast.AST) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        complexity = 1  # åŸºç¡€å¤æ‚åº¦

        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(node, ast.ExceptHandler):
                complexity += 1
            elif isinstance(node, (ast.And, ast.Or)):
                complexity += 1

        return complexity

    def _analyze_function_lengths(self, tree: ast.AST) -> Dict[str, int]:
        """åˆ†æå‡½æ•°é•¿åº¦"""
        function_lengths = {}

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                # è®¡ç®—å‡½æ•°è¡Œæ•°
                if hasattr(node, 'end_lineno') and hasattr(node, 'lineno'):
                    length = node.end_lineno - node.lineno + 1
                    function_lengths[node.name] = length

        return function_lengths

    def _check_imports(self, tree: ast.AST) -> List[Dict]:
        """æ£€æŸ¥å¯¼å…¥è¯­å¥"""
        issues = []
        imports = []

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)

        # æ£€æŸ¥æœªä½¿ç”¨çš„å¯¼å…¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„åˆ†æ

        # æ£€æŸ¥å±é™©çš„å¯¼å…¥
        dangerous_imports = ['os', 'subprocess', 'eval', 'exec']
        for imp in imports:
            if imp in dangerous_imports:
                issues.append({
                    'type': 'security',
                    'severity': 'medium',
                    'message': f'Potentially dangerous import: {imp}',
                    'suggestion': 'Review the usage of this import for security implications'
                })

        return issues

    def _check_naming_conventions(self, tree: ast.AST) -> List[Dict]:
        """æ£€æŸ¥å‘½åè§„èŒƒ"""
        issues = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                if not re.match(r'^[a-z_][a-z0-9_]*$', node.name):
                    issues.append({
                        'type': 'style',
                        'severity': 'low',
                        'message': f'Function name {node.name} does not follow snake_case convention',
                        'suggestion': 'Use snake_case for function names'
                    })

            elif isinstance(node, ast.ClassDef):
                if not re.match(r'^[A-Z][a-zA-Z0-9]*$', node.name):
                    issues.append({
                        'type': 'style',
                        'severity': 'low',
                        'message': f'Class name {node.name} does not follow PascalCase convention',
                        'suggestion': 'Use PascalCase for class names'
                    })

        return issues

    def _check_security_issues(self, content: str) -> List[Dict]:
        """æ£€æŸ¥å®‰å…¨é—®é¢˜"""
        issues = []

        # æ£€æŸ¥SQLæ³¨å…¥é£é™©
        sql_patterns = [
            r'execute\s*\(\s*["\'].*%.*["\']',
            r'cursor\.execute\s*\(\s*["\'].*\+.*["\']',
            r'query\s*=.*\+.*'
        ]

        for pattern in sql_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                issues.append({
                    'type': 'security',
                    'severity': 'high',
                    'message': 'Potential SQL injection vulnerability',
                    'suggestion': 'Use parameterized queries instead of string concatenation'
                })

        # æ£€æŸ¥ç¡¬ç¼–ç å¯†ç 
        password_patterns = [
            r'password\s*=\s*["\'][^"\']+["\']',
            r'pwd\s*=\s*["\'][^"\']+["\']',
            r'secret\s*=\s*["\'][^"\']+["\']'
        ]

        for pattern in password_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                issues.append({
                    'type': 'security',
                    'severity': 'high',
                    'message': 'Hardcoded password or secret detected',
                    'suggestion': 'Use environment variables or secure configuration files'
                })

        return issues

    def _check_performance_issues(self, tree: ast.AST, content: str) -> List[Dict]:
        """æ£€æŸ¥æ€§èƒ½é—®é¢˜"""
        issues = []

        # æ£€æŸ¥å¾ªç¯ä¸­çš„å­—ç¬¦ä¸²æ‹¼æ¥
        for node in ast.walk(tree):
            if isinstance(node, (ast.For, ast.While)):
                for child in ast.walk(node):
                    if isinstance(child, ast.AugAssign) and isinstance(child.op, ast.Add):
                        if isinstance(child.target, ast.Name):
                            issues.append({
                                'type': 'performance',
                                'severity': 'medium',
                                'message': 'String concatenation in loop detected',
                                'suggestion': 'Use list.append() and join() for better performance'
                            })

        # æ£€æŸ¥å…¨å±€å˜é‡çš„ä½¿ç”¨
        global_vars = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Global):
                global_vars.extend(node.names)

        if global_vars:
            issues.append({
                'type': 'maintainability',
                'severity': 'medium',
                'message': f'Global variables used: {", ".join(global_vars)}',
                'suggestion': 'Consider using function parameters or class attributes instead'
            })

        return issues

    def _generate_python_recommendations(self, issues: List[Dict]) -> List[str]:
        """ç”ŸæˆPythonç‰¹å®šçš„å»ºè®®"""
        recommendations = []

        issue_types = [issue['type'] for issue in issues]

        if 'complexity' in issue_types:
            recommendations.append(
                "Consider using design patterns like Strategy or Command to reduce complexity"
            )

        if 'security' in issue_types:
            recommendations.append(
                "Run security linters like bandit to identify additional security issues"
            )

        if 'performance' in issue_types:
            recommendations.append(
                "Profile your code with cProfile to identify performance bottlenecks"
            )

        if 'style' in issue_types:
            recommendations.append(
                "Use tools like black and flake8 to automatically format and check code style"
            )

        return recommendations

    async def _identify_priority_issues(self, issues: List[Dict]) -> List[Dict]:
        """è¯†åˆ«ä¼˜å…ˆçº§é—®é¢˜"""
        priority_issues = []

        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}
        sorted_issues = sorted(issues, key=lambda x: severity_order.get(x.get('severity', 'low'), 3))

        # å–å‰5ä¸ªæœ€ä¸¥é‡çš„é—®é¢˜
        priority_issues = sorted_issues[:5]

        return priority_issues

    async def _generate_review_report(self, review_results: Dict) -> str:
        """ç”Ÿæˆå®¡æŸ¥æŠ¥å‘Š"""
        issues = review_results.get('issues', [])
        recommendations = review_results.get('recommendations', [])
        metrics = review_results.get('metrics', {})
        priority_issues = review_results.get('priority_issues', [])

        report = "# Code Review Report\n\n"

        # æ€»ç»“
        report += f"## Summary\n"
        report += f"- Total issues found: {len(issues)}\n"
        report += f"- Critical issues: {len([i for i in issues if i.get('severity') == 'critical'])}\n"
        report += f"- High priority issues: {len([i for i in issues if i.get('severity') == 'high'])}\n"
        report += f"- Medium priority issues: {len([i for i in issues if i.get('severity') == 'medium'])}\n"
        report += f"- Low priority issues: {len([i for i in issues if i.get('severity') == 'low'])}\n\n"

        # ä¼˜å…ˆçº§é—®é¢˜
        if priority_issues:
            report += "## Priority Issues\n\n"
            for i, issue in enumerate(priority_issues, 1):
                report += f"{i}. **{issue.get('type', 'Unknown').title()}** ({issue.get('severity', 'unknown')})\n"
                report += f"   - {issue.get('message', 'No message')}\n"
                if issue.get('suggestion'):
                    report += f"   - Suggestion: {issue['suggestion']}\n"
                report += "\n"

        # æŒ‡æ ‡
        if metrics:
            report += "## Code Metrics\n\n"
            for file_path, file_metrics in metrics.items():
                report += f"### {file_path}\n"
                for metric, value in file_metrics.items():
                    report += f"- {metric.replace('_', ' ').title()}: {value}\n"
                report += "\n"

        # å»ºè®®
        if recommendations:
            report += "## Recommendations\n\n"
            for i, rec in enumerate(recommendations, 1):
                report += f"{i}. {rec}\n"
            report += "\n"

        return report

# å¾®ä»£ç†æ³¨å†Œ
def register_code_review_microagent():
    """æ³¨å†Œä»£ç å®¡æŸ¥å¾®ä»£ç†"""
    microagent = CodeReviewMicroagent()
    return {
        'name': microagent.name,
        'description': microagent.description,
        'triggers': microagent.triggers,
        'process_request': microagent.process_request,
        'expertise_areas': microagent.expertise_areas
    }
```

## ğŸ“Š å­¦ä¹ è¿›åº¦è·Ÿè¸ª

| æ‰©å±•é¢†åŸŸ | è®¾è®¡é˜¶æ®µ | å¼€å‘é˜¶æ®µ | æµ‹è¯•é˜¶æ®µ | é›†æˆé˜¶æ®µ | å®Œæˆåº¦ |
|---------|----------|----------|----------|----------|--------|
| è‡ªå®šä¹‰å·¥å…· | â³ | â³ | â³ | â³ | 0% |
| å¾®ä»£ç†ç³»ç»Ÿ | â³ | â³ | â³ | â³ | 0% |
| è¿è¡Œæ—¶æ‰©å±• | â³ | â³ | â³ | â³ | 0% |
| æ’ä»¶ç³»ç»Ÿ | â³ | â³ | â³ | â³ | 0% |
| ç³»ç»Ÿé›†æˆ | â³ | â³ | â³ | â³ | 0% |

## ğŸ¯ æ‰©å±•å¼€å‘æœ€ä½³å®è·µ

### 1. å·¥å…·å¼€å‘åŸåˆ™
- **å•ä¸€èŒè´£**ï¼šæ¯ä¸ªå·¥å…·ä¸“æ³¨äºä¸€ä¸ªç‰¹å®šåŠŸèƒ½
- **æ¥å£ä¸€è‡´**ï¼šéµå¾ªç»Ÿä¸€çš„å·¥å…·æ¥å£è§„èŒƒ
- **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
- **æ€§èƒ½ä¼˜åŒ–**ï¼šè€ƒè™‘å·¥å…·çš„æ‰§è¡Œæ•ˆç‡å’Œèµ„æºä½¿ç”¨

### 2. å¾®ä»£ç†è®¾è®¡åŸåˆ™
- **é¢†åŸŸä¸“ä¸šåŒ–**ï¼šä¸“æ³¨äºç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†
- **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šèƒ½å¤Ÿç†è§£å’Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯
- **åŠ¨æ€é€‚åº”**ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚åŠ¨æ€è°ƒæ•´è¡Œä¸º
- **çŸ¥è¯†æ›´æ–°**ï¼šæ”¯æŒçŸ¥è¯†åº“çš„æŒç»­æ›´æ–°

### 3. è¿è¡Œæ—¶æ‰©å±•åŸåˆ™
- **å®‰å…¨éš”ç¦»**ï¼šç¡®ä¿è¿è¡Œæ—¶ç¯å¢ƒçš„å®‰å…¨æ€§
- **èµ„æºç®¡ç†**ï¼šæœ‰æ•ˆç®¡ç†è®¡ç®—èµ„æºå’Œå†…å­˜
- **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæ°´å¹³å’Œå‚ç›´æ‰©å±•
- **ç›‘æ§é›†æˆ**ï¼šå†…ç½®ç›‘æ§å’Œæ—¥å¿—åŠŸèƒ½

## â¡ï¸ ä¸‹ä¸€é˜¶æ®µ

å®Œæˆæœ¬é˜¶æ®µå­¦ä¹ åï¼Œä½ åº”è¯¥ï¼š
- èƒ½å¤Ÿå¼€å‘å¤æ‚çš„ç³»ç»Ÿæ‰©å±•
- æŒæ¡æ’ä»¶åŒ–æ¶æ„è®¾è®¡
- å…·å¤‡ç³»ç»Ÿé›†æˆèƒ½åŠ›
- ä¸ºç”Ÿäº§éƒ¨ç½²åšå¥½å‡†å¤‡

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬è¿›å…¥ [é˜¶æ®µäº”ï¼šç”Ÿäº§éƒ¨ç½²](../stage5-production/) å§ï¼
