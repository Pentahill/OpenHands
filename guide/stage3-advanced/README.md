# é˜¶æ®µä¸‰ï¼šé«˜çº§å¼€å‘ï¼ˆ3-4å‘¨ï¼‰

## ğŸ¯ å­¦ä¹ ç›®æ ‡

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ å°†ï¼š
- æŒæ¡å‰ç«¯Reactå¼€å‘çš„é«˜çº§æŠ€å·§
- æ·±å…¥ç†è§£åç«¯FastAPIæ¶æ„å’Œå¼€å‘
- å­¦ä¹ LLMé›†æˆå’Œä¼˜åŒ–æŠ€æœ¯
- æŒæ¡æ€§èƒ½è°ƒä¼˜å’Œç›‘æ§æŠ€æœ¯
- èƒ½å¤Ÿå¼€å‘å®Œæ•´çš„åŠŸèƒ½æ¨¡å—

## ğŸ“‹ å­¦ä¹ æ¸…å•

### ç¬¬1-5å¤©ï¼šå‰ç«¯å¼€å‘æ·±å…¥
- [ ] æŒæ¡React 19çš„æ–°ç‰¹æ€§å’Œæœ€ä½³å®è·µ
- [ ] æ·±å…¥ç†è§£Redux ToolkitçŠ¶æ€ç®¡ç†
- [ ] å­¦ä¹ TanStack Queryæ•°æ®è·å–å’Œç¼“å­˜
- [ ] æŒæ¡Socket.IOå®æ—¶é€šä¿¡
- [ ] ç†è§£Monaco Editoré›†æˆå’Œå®šåˆ¶

### ç¬¬6-10å¤©ï¼šåç«¯APIå¼€å‘
- [ ] æ·±å…¥å­¦ä¹ FastAPIæ¡†æ¶å’Œå¼‚æ­¥ç¼–ç¨‹
- [ ] ç†è§£ä¾èµ–æ³¨å…¥å’Œä¸­é—´ä»¶æœºåˆ¶
- [ ] æŒæ¡WebSocketå’ŒSSEå®ç°
- [ ] å­¦ä¹ æ•°æ®éªŒè¯å’Œåºåˆ—åŒ–
- [ ] å®ç°RESTful APIè®¾è®¡

### ç¬¬11-15å¤©ï¼šLLMé›†æˆä¼˜åŒ–
- [ ] æ·±å…¥ç†è§£LiteLLMå¤šæ¨¡å‹æ”¯æŒ
- [ ] å­¦ä¹ æç¤ºå·¥ç¨‹å’Œä¼˜åŒ–æŠ€å·§
- [ ] æŒæ¡å‡½æ•°è°ƒç”¨å’Œå·¥å…·ä½¿ç”¨
- [ ] ç†è§£æµå¼å“åº”å’Œå¼‚æ­¥å¤„ç†
- [ ] å®ç°è‡ªå®šä¹‰LLMé›†æˆ

### ç¬¬16-20å¤©ï¼šæ€§èƒ½è°ƒä¼˜
- [ ] åˆ†æç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆ
- [ ] ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œç¼“å­˜
- [ ] å®ç°å¼‚æ­¥å¤„ç†å’Œå¹¶å‘ä¼˜åŒ–
- [ ] å­¦ä¹ å†…å­˜ç®¡ç†å’Œåƒåœ¾å›æ”¶
- [ ] æŒæ¡è´Ÿè½½æµ‹è¯•å’Œæ€§èƒ½ç›‘æ§

### ç¬¬21-28å¤©ï¼šç»¼åˆé¡¹ç›®å¼€å‘
- [ ] è®¾è®¡å’Œå®ç°å®Œæ•´åŠŸèƒ½æ¨¡å—
- [ ] é›†æˆå‰åç«¯å¼€å‘
- [ ] å®ç°æµ‹è¯•å’Œæ–‡æ¡£
- [ ] æ€§èƒ½ä¼˜åŒ–å’Œéƒ¨ç½²å‡†å¤‡
- [ ] å‡†å¤‡è¿›å…¥ä¸‹ä¸€é˜¶æ®µ

## ğŸ“š æ ¸å¿ƒæŠ€æœ¯æ ˆæ·±å…¥

### 1. å‰ç«¯æŠ€æœ¯æ ˆ
```
React 19
â”œâ”€â”€ æ–°ç‰¹æ€§
â”‚   â”œâ”€â”€ Server Components
â”‚   â”œâ”€â”€ Concurrent Features
â”‚   â”œâ”€â”€ Automatic Batching
â”‚   â””â”€â”€ Suspense Improvements
â”œâ”€â”€ çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ Redux Toolkit
â”‚   â”œâ”€â”€ RTK Query
â”‚   â””â”€â”€ Context API
â”œâ”€â”€ æ•°æ®è·å–
â”‚   â”œâ”€â”€ TanStack Query
â”‚   â”œâ”€â”€ SWRæ¨¡å¼
â”‚   â””â”€â”€ ç¼“å­˜ç­–ç•¥
â””â”€â”€ UIç»„ä»¶
    â”œâ”€â”€ HeroUI
    â”œâ”€â”€ Tailwind CSS
    â””â”€â”€ Framer Motion
```

### 2. åç«¯æŠ€æœ¯æ ˆ
```
FastAPI
â”œâ”€â”€ æ ¸å¿ƒç‰¹æ€§
â”‚   â”œâ”€â”€ å¼‚æ­¥æ”¯æŒ
â”‚   â”œâ”€â”€ è‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆ
â”‚   â”œâ”€â”€ ç±»å‹éªŒè¯
â”‚   â””â”€â”€ ä¾èµ–æ³¨å…¥
â”œâ”€â”€ æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ Pydanticæ¨¡å‹
â”‚   â”œâ”€â”€ åºåˆ—åŒ–/ååºåˆ—åŒ–
â”‚   â””â”€â”€ æ•°æ®éªŒè¯
â”œâ”€â”€ å®æ—¶é€šä¿¡
â”‚   â”œâ”€â”€ WebSocket
â”‚   â”œâ”€â”€ Server-Sent Events
â”‚   â””â”€â”€ Socket.IO
â””â”€â”€ ä¸­é—´ä»¶
    â”œâ”€â”€ CORSå¤„ç†
    â”œâ”€â”€ è®¤è¯æˆæƒ
    â””â”€â”€ é”™è¯¯å¤„ç†
```

### 3. LLMé›†æˆæŠ€æœ¯
```
LiteLLM
â”œâ”€â”€ å¤šæ¨¡å‹æ”¯æŒ
â”‚   â”œâ”€â”€ OpenAI
â”‚   â”œâ”€â”€ Anthropic
â”‚   â”œâ”€â”€ Google
â”‚   â””â”€â”€ æœ¬åœ°æ¨¡å‹
â”œâ”€â”€ åŠŸèƒ½ç‰¹æ€§
â”‚   â”œâ”€â”€ å‡½æ•°è°ƒç”¨
â”‚   â”œâ”€â”€ æµå¼å“åº”
â”‚   â”œâ”€â”€ æç¤ºç¼“å­˜
â”‚   â””â”€â”€ é”™è¯¯é‡è¯•
â”œâ”€â”€ ä¼˜åŒ–æŠ€æœ¯
â”‚   â”œâ”€â”€ æ‰¹å¤„ç†
â”‚   â”œâ”€â”€ å¹¶å‘æ§åˆ¶
â”‚   â”œâ”€â”€ è´Ÿè½½å‡è¡¡
â”‚   â””â”€â”€ æˆæœ¬ä¼˜åŒ–
â””â”€â”€ è‡ªå®šä¹‰é›†æˆ
    â”œâ”€â”€ è‡ªå®šä¹‰æä¾›å•†
    â”œâ”€â”€ æ¨¡å‹é€‚é…å™¨
    â””â”€â”€ ä¸­é—´ä»¶æ‰©å±•
```

## ğŸ› ï¸ å®è·µé¡¹ç›®

### é¡¹ç›®1ï¼šé«˜çº§å‰ç«¯ç»„ä»¶å¼€å‘
åˆ›å»ºä¸€ä¸ªæ™ºèƒ½ä»£ç ç¼–è¾‘å™¨ç»„ä»¶ï¼š

```typescript
// SmartCodeEditor.tsx
import React, { useCallback, useEffect, useRef } from 'react';
import { Editor } from '@monaco-editor/react';
import { useQuery, useMutation } from '@tanstack/react-query';
import { useSocket } from '../hooks/useSocket';

interface SmartCodeEditorProps {
  language: string;
  initialValue?: string;
  onSave?: (content: string) => void;
  enableAIAssist?: boolean;
}

export const SmartCodeEditor: React.FC<SmartCodeEditorProps> = ({
  language,
  initialValue = '',
  onSave,
  enableAIAssist = true
}) => {
  const editorRef = useRef<any>(null);
  const socket = useSocket();

  // è·å–è¯­è¨€é…ç½®
  const { data: languageConfig } = useQuery({
    queryKey: ['language-config', language],
    queryFn: () => fetchLanguageConfig(language)
  });

  // AIä»£ç å»ºè®®
  const { mutate: getAISuggestions } = useMutation({
    mutationFn: (context: string) => getCodeSuggestions(context),
    onSuccess: (suggestions) => {
      // æ˜¾ç¤ºAIå»ºè®®
      showSuggestions(suggestions);
    }
  });

  // ç¼–è¾‘å™¨æŒ‚è½½
  const handleEditorDidMount = useCallback((editor: any, monaco: any) => {
    editorRef.current = editor;

    // é…ç½®ç¼–è¾‘å™¨
    setupEditor(editor, monaco, languageConfig);

    // æ³¨å†Œå¿«æ·é”®
    editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.KeyS, () => {
      const content = editor.getValue();
      onSave?.(content);
    });

    // AIè¾…åŠ©
    if (enableAIAssist) {
      setupAIAssist(editor, monaco);
    }
  }, [languageConfig, onSave, enableAIAssist]);

  // å†…å®¹å˜åŒ–å¤„ç†
  const handleEditorChange = useCallback((value: string | undefined) => {
    if (enableAIAssist && value) {
      // å»¶è¿Ÿè·å–AIå»ºè®®
      debounceGetSuggestions(value);
    }
  }, [enableAIAssist]);

  // å®æ—¶åä½œ
  useEffect(() => {
    if (socket) {
      socket.on('code-change', (change: any) => {
        // åº”ç”¨è¿œç¨‹æ›´æ”¹
        applyRemoteChange(change);
      });

      return () => {
        socket.off('code-change');
      };
    }
  }, [socket]);

  return (
    <div className="smart-code-editor">
      <Editor
        height="400px"
        language={language}
        value={initialValue}
        onMount={handleEditorDidMount}
        onChange={handleEditorChange}
        options={{
          theme: 'vs-dark',
          fontSize: 14,
          minimap: { enabled: false },
          scrollBeyondLastLine: false,
          automaticLayout: true,
          suggestOnTriggerCharacters: true,
          quickSuggestions: true,
          wordBasedSuggestions: true
        }}
      />
    </div>
  );
};

// è¾…åŠ©å‡½æ•°
const setupEditor = (editor: any, monaco: any, config: any) => {
  // é…ç½®è¯­è¨€ç‰¹æ€§
  if (config) {
    monaco.languages.setLanguageConfiguration(config.language, config.configuration);
    monaco.languages.setMonarchTokensProvider(config.language, config.tokenizer);
  }
};

const setupAIAssist = (editor: any, monaco: any) => {
  // æ³¨å†ŒAIå»ºè®®æä¾›è€…
  monaco.languages.registerCompletionItemProvider('typescript', {
    provideCompletionItems: async (model: any, position: any) => {
      const context = model.getValueInRange({
        startLineNumber: Math.max(1, position.lineNumber - 10),
        startColumn: 1,
        endLineNumber: position.lineNumber,
        endColumn: position.column
      });

      const suggestions = await getCodeSuggestions(context);
      return { suggestions };
    }
  });
};
```

### é¡¹ç›®2ï¼šé«˜çº§åç«¯APIå¼€å‘
åˆ›å»ºä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡å¤„ç†APIï¼š

```python
# smart_task_api.py
from fastapi import FastAPI, Depends, HTTPException, BackgroundTasks
from fastapi.websockets import WebSocket, WebSocketDisconnect
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import asyncio
import json
from datetime import datetime

app = FastAPI(title="Smart Task API", version="1.0.0")

class TaskRequest(BaseModel):
    task_type: str = Field(..., description="ä»»åŠ¡ç±»å‹")
    content: str = Field(..., description="ä»»åŠ¡å†…å®¹")
    priority: int = Field(default=1, ge=1, le=5, description="ä¼˜å…ˆçº§")
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="å…ƒæ•°æ®")

class TaskResponse(BaseModel):
    task_id: str
    status: str
    result: Optional[Dict[str, Any]] = None
    created_at: datetime
    updated_at: datetime

class TaskManager:
    def __init__(self):
        self.tasks: Dict[str, Dict] = {}
        self.task_queue = asyncio.Queue()
        self.workers = []
        self.websocket_connections: List[WebSocket] = []

    async def start_workers(self, num_workers: int = 3):
        """å¯åŠ¨ä»»åŠ¡å¤„ç†å·¥ä½œè€…"""
        for i in range(num_workers):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)

    async def _worker(self, worker_id: str):
        """ä»»åŠ¡å¤„ç†å·¥ä½œè€…"""
        while True:
            try:
                task = await self.task_queue.get()
                await self._process_task(task, worker_id)
                self.task_queue.task_done()
            except Exception as e:
                print(f"Worker {worker_id} error: {e}")

    async def _process_task(self, task: Dict, worker_id: str):
        """å¤„ç†å…·ä½“ä»»åŠ¡"""
        task_id = task['id']
        task_type = task['type']

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        self.tasks[task_id]['status'] = 'processing'
        self.tasks[task_id]['worker'] = worker_id
        await self._broadcast_task_update(task_id)

        try:
            # æ ¹æ®ä»»åŠ¡ç±»å‹å¤„ç†
            if task_type == 'code_analysis':
                result = await self._analyze_code(task['content'])
            elif task_type == 'text_generation':
                result = await self._generate_text(task['content'])
            elif task_type == 'data_processing':
                result = await self._process_data(task['content'])
            else:
                raise ValueError(f"Unknown task type: {task_type}")

            # æ›´æ–°ä»»åŠ¡ç»“æœ
            self.tasks[task_id]['status'] = 'completed'
            self.tasks[task_id]['result'] = result
            self.tasks[task_id]['updated_at'] = datetime.now()

        except Exception as e:
            self.tasks[task_id]['status'] = 'failed'
            self.tasks[task_id]['error'] = str(e)
            self.tasks[task_id]['updated_at'] = datetime.now()

        await self._broadcast_task_update(task_id)

    async def _broadcast_task_update(self, task_id: str):
        """å¹¿æ’­ä»»åŠ¡æ›´æ–°"""
        task_data = self.tasks[task_id]
        message = {
            'type': 'task_update',
            'task_id': task_id,
            'status': task_data['status'],
            'updated_at': task_data['updated_at'].isoformat()
        }

        # å‘é€ç»™æ‰€æœ‰è¿æ¥çš„WebSocketå®¢æˆ·ç«¯
        for websocket in self.websocket_connections[:]:
            try:
                await websocket.send_text(json.dumps(message))
            except:
                self.websocket_connections.remove(websocket)

    async def _analyze_code(self, code: str) -> Dict[str, Any]:
        """ä»£ç åˆ†æä»»åŠ¡"""
        # æ¨¡æ‹Ÿå¼‚æ­¥å¤„ç†
        await asyncio.sleep(2)

        # è¿™é‡Œå¯ä»¥é›†æˆçœŸå®çš„ä»£ç åˆ†æå·¥å…·
        return {
            'lines': len(code.split('\n')),
            'complexity': 'medium',
            'suggestions': ['Add type hints', 'Improve error handling']
        }

    async def _generate_text(self, prompt: str) -> Dict[str, Any]:
        """æ–‡æœ¬ç”Ÿæˆä»»åŠ¡"""
        # æ¨¡æ‹ŸLLMè°ƒç”¨
        await asyncio.sleep(3)

        return {
            'generated_text': f"Generated response for: {prompt}",
            'tokens_used': 150,
            'model': 'gpt-4'
        }

    async def _process_data(self, data: str) -> Dict[str, Any]:
        """æ•°æ®å¤„ç†ä»»åŠ¡"""
        await asyncio.sleep(1)

        return {
            'processed_items': 100,
            'processing_time': '1.2s',
            'status': 'success'
        }

# å…¨å±€ä»»åŠ¡ç®¡ç†å™¨
task_manager = TaskManager()

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    await task_manager.start_workers(3)

@app.post("/tasks", response_model=TaskResponse)
async def create_task(
    task_request: TaskRequest,
    background_tasks: BackgroundTasks
):
    """åˆ›å»ºæ–°ä»»åŠ¡"""
    import uuid

    task_id = str(uuid.uuid4())
    now = datetime.now()

    # åˆ›å»ºä»»åŠ¡è®°å½•
    task_data = {
        'id': task_id,
        'type': task_request.task_type,
        'content': task_request.content,
        'priority': task_request.priority,
        'metadata': task_request.metadata or {},
        'status': 'pending',
        'created_at': now,
        'updated_at': now
    }

    task_manager.tasks[task_id] = task_data

    # æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—
    await task_manager.task_queue.put(task_data)

    return TaskResponse(
        task_id=task_id,
        status='pending',
        created_at=now,
        updated_at=now
    )

@app.get("/tasks/{task_id}", response_model=TaskResponse)
async def get_task(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in task_manager.tasks:
        raise HTTPException(status_code=404, detail="Task not found")

    task_data = task_manager.tasks[task_id]
    return TaskResponse(
        task_id=task_id,
        status=task_data['status'],
        result=task_data.get('result'),
        created_at=task_data['created_at'],
        updated_at=task_data['updated_at']
    )

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocketè¿æ¥ç«¯ç‚¹"""
    await websocket.accept()
    task_manager.websocket_connections.append(websocket)

    try:
        while True:
            # ä¿æŒè¿æ¥æ´»è·ƒ
            await websocket.receive_text()
    except WebSocketDisconnect:
        task_manager.websocket_connections.remove(websocket)

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "active_tasks": len([t for t in task_manager.tasks.values() if t['status'] == 'processing']),
        "pending_tasks": task_manager.task_queue.qsize()
    }
```

### é¡¹ç›®3ï¼šLLMé›†æˆä¼˜åŒ–
åˆ›å»ºä¸€ä¸ªæ™ºèƒ½LLMç®¡ç†å™¨ï¼š

```python
# smart_llm_manager.py
import asyncio
import time
from typing import Dict, List, Optional, AsyncGenerator
from dataclasses import dataclass
from enum import Enum
import litellm
from litellm import completion, acompletion
import json

class ModelProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    LOCAL = "local"

@dataclass
class ModelConfig:
    name: str
    provider: ModelProvider
    max_tokens: int
    cost_per_token: float
    supports_functions: bool
    supports_streaming: bool

class SmartLLMManager:
    def __init__(self):
        self.models: Dict[str, ModelConfig] = {}
        self.usage_stats: Dict[str, Dict] = {}
        self.rate_limits: Dict[str, Dict] = {}
        self.load_balancer = LoadBalancer()

        # åˆå§‹åŒ–æ¨¡å‹é…ç½®
        self._initialize_models()

    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹é…ç½®"""
        self.models.update({
            "gpt-4": ModelConfig(
                name="gpt-4",
                provider=ModelProvider.OPENAI,
                max_tokens=8192,
                cost_per_token=0.00003,
                supports_functions=True,
                supports_streaming=True
            ),
            "claude-3-sonnet": ModelConfig(
                name="claude-3-sonnet-20240229",
                provider=ModelProvider.ANTHROPIC,
                max_tokens=4096,
                cost_per_token=0.000015,
                supports_functions=True,
                supports_streaming=True
            ),
            "gemini-pro": ModelConfig(
                name="gemini-pro",
                provider=ModelProvider.GOOGLE,
                max_tokens=2048,
                cost_per_token=0.00001,
                supports_functions=False,
                supports_streaming=True
            )
        })

    async def smart_completion(
        self,
        messages: List[Dict],
        task_type: str = "general",
        requirements: Optional[Dict] = None
    ) -> Dict:
        """æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œå®Œæˆ"""

        # é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
        model_name = await self._select_best_model(
            messages, task_type, requirements
        )

        # ä¼˜åŒ–æç¤º
        optimized_messages = await self._optimize_prompt(messages, model_name)

        # æ‰§è¡Œå®Œæˆ
        try:
            response = await self._execute_completion(
                model_name, optimized_messages, requirements
            )

            # è®°å½•ä½¿ç”¨ç»Ÿè®¡
            await self._record_usage(model_name, response)

            return response

        except Exception as e:
            # æ•…éšœè½¬ç§»
            return await self._fallback_completion(
                messages, task_type, requirements, str(e)
            )

    async def _select_best_model(
        self,
        messages: List[Dict],
        task_type: str,
        requirements: Optional[Dict]
    ) -> str:
        """é€‰æ‹©æœ€ä½³æ¨¡å‹"""

        # åˆ†æä»»åŠ¡ç‰¹å¾
        task_features = await self._analyze_task(messages, task_type)

        # è¯„åˆ†æ¨¡å‹
        model_scores = {}
        for model_name, config in self.models.items():
            score = await self._score_model(config, task_features, requirements)
            model_scores[model_name] = score

        # è€ƒè™‘è´Ÿè½½å‡è¡¡
        balanced_scores = await self.load_balancer.adjust_scores(model_scores)

        # é€‰æ‹©æœ€é«˜åˆ†æ¨¡å‹
        best_model = max(balanced_scores.items(), key=lambda x: x[1])[0]
        return best_model

    async def _analyze_task(self, messages: List[Dict], task_type: str) -> Dict:
        """åˆ†æä»»åŠ¡ç‰¹å¾"""
        total_tokens = sum(len(msg.get('content', '').split()) for msg in messages)

        features = {
            'token_count': total_tokens,
            'complexity': self._estimate_complexity(messages),
            'requires_functions': 'function_call' in str(messages),
            'requires_streaming': task_type in ['code_generation', 'long_text'],
            'task_type': task_type
        }

        return features

    async def _score_model(
        self,
        config: ModelConfig,
        features: Dict,
        requirements: Optional[Dict]
    ) -> float:
        """ä¸ºæ¨¡å‹è¯„åˆ†"""
        score = 100.0

        # åŸºäºæˆæœ¬çš„è¯„åˆ†
        cost_factor = 1.0 / (config.cost_per_token * 1000000)  # è½¬æ¢ä¸ºæ¯ç™¾ä¸‡token
        score *= cost_factor

        # åŸºäºèƒ½åŠ›çš„è¯„åˆ†
        if features['requires_functions'] and not config.supports_functions:
            score *= 0.1  # ä¸¥é‡é™åˆ†

        if features['requires_streaming'] and not config.supports_streaming:
            score *= 0.8

        # åŸºäºè´Ÿè½½çš„è¯„åˆ†
        current_load = await self._get_model_load(config.name)
        load_factor = max(0.1, 1.0 - current_load)
        score *= load_factor

        # åŸºäºå†å²æ€§èƒ½çš„è¯„åˆ†
        performance_factor = await self._get_performance_factor(config.name)
        score *= performance_factor

        return score

    async def _optimize_prompt(
        self,
        messages: List[Dict],
        model_name: str
    ) -> List[Dict]:
        """ä¼˜åŒ–æç¤º"""
        config = self.models[model_name]

        # æ ¹æ®æ¨¡å‹ç‰¹ç‚¹ä¼˜åŒ–
        if config.provider == ModelProvider.ANTHROPIC:
            # Claudeå–œæ¬¢æ›´ç»“æ„åŒ–çš„æç¤º
            return self._optimize_for_claude(messages)
        elif config.provider == ModelProvider.OPENAI:
            # GPT-4ä¼˜åŒ–
            return self._optimize_for_gpt(messages)
        else:
            return messages

    async def _execute_completion(
        self,
        model_name: str,
        messages: List[Dict],
        requirements: Optional[Dict]
    ) -> Dict:
        """æ‰§è¡Œæ¨¡å‹å®Œæˆ"""
        config = self.models[model_name]

        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": model_name,
            "messages": messages,
            "max_tokens": min(requirements.get('max_tokens', 1000), config.max_tokens),
            "temperature": requirements.get('temperature', 0.7)
        }

        # æ·»åŠ å‡½æ•°è°ƒç”¨æ”¯æŒ
        if requirements and requirements.get('functions') and config.supports_functions:
            params['functions'] = requirements['functions']
            params['function_call'] = requirements.get('function_call', 'auto')

        # æ‰§è¡Œè¯·æ±‚
        if requirements and requirements.get('stream', False) and config.supports_streaming:
            return await self._stream_completion(params)
        else:
            response = await acompletion(**params)
            return response

    async def _stream_completion(self, params: Dict) -> AsyncGenerator[Dict, None]:
        """æµå¼å®Œæˆ"""
        params['stream'] = True

        async for chunk in await acompletion(**params):
            yield chunk

    async def _record_usage(self, model_name: str, response: Dict):
        """è®°å½•ä½¿ç”¨ç»Ÿè®¡"""
        if model_name not in self.usage_stats:
            self.usage_stats[model_name] = {
                'total_requests': 0,
                'total_tokens': 0,
                'total_cost': 0.0,
                'avg_response_time': 0.0,
                'error_rate': 0.0
            }

        stats = self.usage_stats[model_name]
        stats['total_requests'] += 1

        if hasattr(response, 'usage'):
            tokens = response.usage.total_tokens
            stats['total_tokens'] += tokens

            config = self.models[model_name]
            cost = tokens * config.cost_per_token
            stats['total_cost'] += cost

class LoadBalancer:
    def __init__(self):
        self.model_loads: Dict[str, float] = {}
        self.request_counts: Dict[str, int] = {}

    async def adjust_scores(self, scores: Dict[str, float]) -> Dict[str, float]:
        """æ ¹æ®è´Ÿè½½è°ƒæ•´åˆ†æ•°"""
        adjusted_scores = {}

        for model_name, score in scores.items():
            load = self.model_loads.get(model_name, 0.0)
            load_factor = max(0.1, 1.0 - load)
            adjusted_scores[model_name] = score * load_factor

        return adjusted_scores

    async def update_load(self, model_name: str, load: float):
        """æ›´æ–°æ¨¡å‹è´Ÿè½½"""
        self.model_loads[model_name] = load

    async def increment_requests(self, model_name: str):
        """å¢åŠ è¯·æ±‚è®¡æ•°"""
        self.request_counts[model_name] = self.request_counts.get(model_name, 0) + 1

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    llm_manager = SmartLLMManager()

    messages = [
        {"role": "user", "content": "è¯·å¸®æˆ‘åˆ†æè¿™æ®µPythonä»£ç çš„æ€§èƒ½é—®é¢˜"}
    ]

    response = await llm_manager.smart_completion(
        messages=messages,
        task_type="code_analysis",
        requirements={
            "max_tokens": 1000,
            "temperature": 0.3,
            "functions": [
                {
                    "name": "analyze_code",
                    "description": "åˆ†æä»£ç æ€§èƒ½",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "issues": {"type": "array", "items": {"type": "string"}},
                            "suggestions": {"type": "array", "items": {"type": "string"}}
                        }
                    }
                }
            ]
        }
    )

    print(response)

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ“Š å­¦ä¹ è¿›åº¦è·Ÿè¸ª

| æŠ€æœ¯é¢†åŸŸ | ç†è®ºå­¦ä¹  | å®è·µå¼€å‘ | é¡¹ç›®å®Œæˆ | æŒæ¡ç¨‹åº¦ |
|---------|----------|----------|----------|----------|
| Reacté«˜çº§å¼€å‘ | â³ | â³ | â³ | 0% |
| FastAPIåç«¯ | â³ | â³ | â³ | 0% |
| LLMé›†æˆä¼˜åŒ– | â³ | â³ | â³ | 0% |
| æ€§èƒ½è°ƒä¼˜ | â³ | â³ | â³ | 0% |
| ç»¼åˆé¡¹ç›® | â³ | â³ | â³ | 0% |

## ğŸ¯ æ ¸å¿ƒæŠ€èƒ½ç›®æ ‡

### å‰ç«¯å¼€å‘æŠ€èƒ½
- [ ] ç†Ÿç»ƒä½¿ç”¨React 19æ–°ç‰¹æ€§
- [ ] æŒæ¡å¤æ‚çŠ¶æ€ç®¡ç†
- [ ] å®ç°é«˜æ€§èƒ½ç»„ä»¶
- [ ] é›†æˆå®æ—¶é€šä¿¡åŠŸèƒ½

### åç«¯å¼€å‘æŠ€èƒ½
- [ ] è®¾è®¡RESTful API
- [ ] å®ç°å¼‚æ­¥å¤„ç†
- [ ] é›†æˆWebSocketé€šä¿¡
- [ ] ä¼˜åŒ–æ•°æ®åº“æ“ä½œ

### LLMé›†æˆæŠ€èƒ½
- [ ] å¤šæ¨¡å‹ç®¡ç†
- [ ] æ™ºèƒ½æ¨¡å‹é€‰æ‹©
- [ ] æç¤ºå·¥ç¨‹ä¼˜åŒ–
- [ ] æˆæœ¬æ§åˆ¶ç­–ç•¥

### æ€§èƒ½ä¼˜åŒ–æŠ€èƒ½
- [ ] æ€§èƒ½ç“¶é¢ˆåˆ†æ
- [ ] ç¼“å­˜ç­–ç•¥è®¾è®¡
- [ ] å¹¶å‘å¤„ç†ä¼˜åŒ–
- [ ] ç›‘æ§å’Œå‘Šè­¦

## ğŸ”§ å¼€å‘å·¥å…·å’ŒæŠ€å·§

### å‰ç«¯å¼€å‘å·¥å…·
- **React DevTools**: ç»„ä»¶è°ƒè¯•å’Œæ€§èƒ½åˆ†æ
- **Redux DevTools**: çŠ¶æ€ç®¡ç†è°ƒè¯•
- **Lighthouse**: æ€§èƒ½å’Œå¯è®¿é—®æ€§æµ‹è¯•
- **Storybook**: ç»„ä»¶å¼€å‘å’Œæµ‹è¯•

### åç«¯å¼€å‘å·¥å…·
- **FastAPIè‡ªåŠ¨æ–‡æ¡£**: äº¤äº’å¼APIæ–‡æ¡£
- **Uvicorn**: é«˜æ€§èƒ½ASGIæœåŠ¡å™¨
- **Pydantic**: æ•°æ®éªŒè¯å’Œåºåˆ—åŒ–
- **SQLAlchemy**: ORMå’Œæ•°æ®åº“æ“ä½œ

### æ€§èƒ½åˆ†æå·¥å…·
- **Chrome DevTools**: å‰ç«¯æ€§èƒ½åˆ†æ
- **cProfile**: Pythonæ€§èƒ½åˆ†æ
- **Memory Profiler**: å†…å­˜ä½¿ç”¨åˆ†æ
- **Load Testing**: å‹åŠ›æµ‹è¯•å·¥å…·

## â¡ï¸ ä¸‹ä¸€é˜¶æ®µ

å®Œæˆæœ¬é˜¶æ®µå­¦ä¹ åï¼Œä½ åº”è¯¥ï¼š
- å…·å¤‡å…¨æ ˆå¼€å‘èƒ½åŠ›
- èƒ½å¤Ÿä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½
- æŒæ¡LLMé›†æˆæŠ€æœ¯
- ä¸ºæ‰©å±•å¼€å‘åšå¥½å‡†å¤‡

å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬è¿›å…¥ [é˜¶æ®µå››ï¼šæ‰©å±•å¼€å‘](../stage4-extension/) å§ï¼
